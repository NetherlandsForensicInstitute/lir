{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1-almM0ZF3D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A practitioner’s guide for building LR systems using a data-driven approach\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA5h9E5iaErB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook accompanies the paper \"From data to a validated score-based LR system: a practitioner’s guide\". The paper offers a guideline to go from a data set to a validated Likelihood Ratio (LR) system in 8 steps. In this notebook, we illustrate each of these steps with working Python code. In the code, we build and validate an LR system for a data set of observations on the elememental concentrations of glass fragments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlweYWrlUhkN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Follow along in the notebook by running the code blocks pressing Shift+Enter on your keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_t3l27ixzrS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The guideline involves creating multiple LR systems at first, and selection of the best LR system in a later step. In this paper we explore two options to compute scores and two options to transform scores to LRs in a two-by-two design, leading to four LR systems.\n",
    "\n",
    "The 'scorer' options are:\n",
    "1. Use the Manhattan distance (defined as the sum of the absolute differences)\n",
    "2. Use a support vector machine, a machine learning model\n",
    "\n",
    "The 'calibrator' options to transform scores to LRs are:\n",
    "<ol type = 'A'>\n",
    "<li>Use a generative approach, namely kernel density estimation</li>\n",
    "<li>Use a discriminative approach, namely logistic regression</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nsabb_dNas5V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hbaU61l9ybL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We start by installing several Python packages. These are standard packages, and the LiR package that we developed and maintain. This package provides a collection of scripts to aid construction and evaluation of LR systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWCy2IyGsuLl",
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Try to keep this in line with the requirements for lir itself\n",
    "! pip install lir==1.2.0\n",
    "! pip install scipy>=1.31\n",
    "! pip install scikit-learn>=1.2\n",
    "! pip install numpy>=1.22\n",
    "# # Additional dependencies, already installed in Colab:\n",
    "# ! pip install pandas seaborn==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXQtB9-VpXdE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import paired_manhattan_distances\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lir\n",
    "from lir.algorithms.bayeserror import ELUBBounder\n",
    "from lir.algorithms.kde import KDECalibrator\n",
    "from lir.algorithms.logistic_regression import LogitCalibrator\n",
    "from lir.plotting import show\n",
    "from lir.transform.distance import AbsDiffTransformer\n",
    "from lir.transform.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7ewjpFgJ3wH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Introduction LiR and sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Mc2-RutKMzn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LiR follows the conventions of the sklearn package, which is the de facto standard in machine learning. This has the advantage that we can directly use all models defined in sklearn and that the code will look familiar to anyone used to sklearn. It has the disadvantage that naming of certain functions and objects may appear counterintuitive to those unfamiliar with sklearn. Sklearn defines objects that can 'fit' on data, and then 'transform' new data or 'predict' probabilities. LiR additionally allows to 'predict_lr'. We will encounter examples of these below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kz9UGBQJ9cH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnsFsxCnZ7Ug",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We download the glass data in the form of a csv file and convert this to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBR8ua6QaJcR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csv_url = 'https://raw.githubusercontent.com/NetherlandsForensicInstitute/elemental_composition_glass/main/duplo.csv'\n",
    "data_set = pd.read_csv(csv_url, delimiter=',').rename(\n",
    "    columns = {'Item': 'Subject', 'Piece': 'Repeat', 'id': 'Id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDMLkAizby_M",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 1. Explore data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpK3F2cD1M7-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data set we will use consists of elemental concentrations of float glass, obtained with LA-ICPMS (a type of mass spectrometer) ([see GitHub repository](https://github.com/NetherlandsForensicInstitute/elemental_composition_glass)). The elemental concentrations are on a log<sub>10</sub> basis, and normalized to the element silicon (Si). For each glass source, which we refer to as *subject*, two measurements are performed. Each time the concentration of ten elements is measured.\n",
    "\n",
    "First, we will explore the type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlC95xzGb7Re",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me5frqI65Vco",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As can be seen there are 13 columns for each observation:\n",
    "- Id: a unique integer for each observation\n",
    "- Subject: a unique integer for each subject\n",
    "- Repeat: for each subject, a unique integer representing the repeated observations on this subject\n",
    "- K39, Ti49, Mn55, Rb85, Sr88, Zr90, Ba137, La139, Ce140, Pb208: the processed concentration of the ten elements. The number behind the element in the column name specifies which isotope of the element was measured. The processed concentrations are all float numbers.\n",
    "\n",
    "Furthermore, we see that there are no non-null values. In other words, there are no missing values.\n",
    "\n",
    "Next, we have a look at the amount of data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4_xz-b-ssqi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of observations: ', data_set['Id'].nunique())\n",
    "print('Number of subjects: ', data_set['Subject'].nunique())\n",
    "print('Maximum number of repeated observations on one subject: ',\n",
    "      data_set['Repeat'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz2uTOprZveI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above you can see the number of glass panes (=distinct sources) and observations.\n",
    "\n",
    "In this guide, we limit the number of items to ensure that we do not run out of RAM during the subsequent calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fSj5yITr5NZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that the sorting is first by item, then by repeat\n",
    "data_set = data_set.sort_values(by=['Subject', 'Repeat'], axis=0, ascending=True)\n",
    "\n",
    "# Only keep the first 400 rows, as the free version of Google Colab does not\n",
    "# have enough RAM available for computations on the full set\n",
    "data_set = data_set.drop(data_set.index[400:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a75K4FgSRP9C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As a check, the code block above can be run again, now showing 400 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn2pEJfx4dMn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each row of the dataframe contains one observation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtxblU794T8X",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(data_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FMbG0Eh43y6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "To get a bit more feeling for the data, we create a histogram of all observations per element together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdQeNxW1z2Y0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Histogram of all measured concentrations:')\n",
    "data_set[['K39', 'Ti49', 'Mn55', 'Rb85', 'Sr88', 'Zr90', 'Ba137', 'La139',\n",
    "          'Ce140', 'Pb208']].hist(figsize=(10,10), layout=(2,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uVcD1E4q3lg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the histograms, we see that there are no distinctive outliers, although both Mn55 and Ba137 show data points with a relatively high value of around 4. As these is no reason to suspect an error, these probably reflect variability in the population and should not be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJhlxzAQwOdO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we select the variables (the 10 elements) and labels (indicating which subject was measured) and convert them to a numpy array. This is the format needed for the rest of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaG0qs_owLkR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "variables = ['K39', 'Ti49', 'Mn55', 'Rb85', 'Sr88', 'Zr90', 'Ba137', 'La139',\n",
    "             'Ce140', 'Pb208']\n",
    "labels = ['Subject']\n",
    "\n",
    "obs = data_set[variables].to_numpy()\n",
    "ids = data_set[labels].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZj9_w6xwqVx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Explore data - a deeper look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0SfjB1EQKPw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at the data more closely, we see that the concentrations for repeat measurements on the same subject are more similar to each other than the concentrations measured for different subjects. To examine this further, we calculate for each subject the standard deviation of its two measurements (a measure of within-source variability). We compare this to the standard deviation of the mean concentrations across all subjects (a measure of between-source variability). We expect the former to be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drE38OKe3HYP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Histograms of standard deviations for the repeat measurements on a subject:')\n",
    "data_set.groupby('Subject')[variables].std().hist(figsize=(10,10), layout=(2,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99sydF9LRY3D",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Between subject standard deviations compared to the maximum within subject standard deviation:')\n",
    "st_devs = pd.DataFrame(data_set.groupby('Subject')[variables].mean().std())\n",
    "st_devs.columns = ['std_between']\n",
    "st_devs['std_within_mean'] = pd.DataFrame(\n",
    "    data_set.groupby('Subject')[variables].std().mean())\n",
    "st_devs['std_within_max'] = pd.DataFrame(\n",
    "    data_set.groupby('Subject')[variables].std().max())\n",
    "st_devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC6-sTCctnD2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that the within-source standard deviations roughly lie between 0 and 0.03 (second column), and between-source standard deviations are much larger (first column). This holds for all elements. In fact, the between objects standard deviation is larger than the largest within standard deviation we encounter (third column). This indicates that an LR system based on this data can be discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPF8z6fSmI8w",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When different variables are considered for calculating an evidential value, it is best when these variables have a correlation coefficient close to zero. If two variables are highly correlated however, they can still both be used in the LR system; their combined within variance is reduced compared to measuring a single variable. If the number of variables is large compared to the number of subjects, we may consider dimension reduction.\n",
    "\n",
    "To see to what extent the concentrations of the different elements are correlated, we create scatterplots plotting the concentrations against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4hBiZOs0bNh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='ticks')\n",
    "sns.pairplot(data_set.reset_index()[variables])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XEXCbkPyCOU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that some of the values are correlated, for example for K39 and Rb85. Most scatterplots seem quite random, indicating a low correlation. This is indeed reflected in the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrBWuk5Ay9Eh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corr = data_set[variables].corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc93A2Xz0nUU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As the level of correlation is not too high, in this notebook we proceed with these data as they are. However, given that some variables are (strongly) correlated, we encourage the reader to experiment with improving the resulting LR system by handling this correlation. For example, by combining these variables or removing some from our data set (a form of dimensionality reduction), or utilizing models that allow for high correlation between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWsc2kRycbW8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 2. Split data in different subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSwQLDeMf8Nd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "As explained in the manuscript, splitting the data is a necessary step in order to evaluate the performance of the model while avoiding overfitting. This means that part of the data will be used to build the model (training and selection sets) and another part is reserved for validation.\n",
    "\n",
    "As shown in the steps below, all three sets should be independent, e.g. consist of disjoint sets of glass objects. First, we set 20% of the objects aside for the validation set, and then we split the remaining objects into a training (80%) and a selection (20%) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCRVmcST_5da",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First we split off 20% from the data as a hold-out validation set (grouped per glass fragment)\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state=1)\n",
    "split = splitter.split(obs, groups=ids)\n",
    "train_select_indices, val_indices = next(split)\n",
    "\n",
    "# Then we split off 20% to use as a test set\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state=1)\n",
    "split = splitter.split(obs[train_select_indices], groups=ids[train_select_indices])\n",
    "train_indices, select_indices = next(split)\n",
    "\n",
    "# We create the train, selection and validation set\n",
    "# 'obs' are the concentrations, 'ids' are the corresponding labels indicating the subject\n",
    "ids_train = ids[train_indices]\n",
    "ids_select = ids[select_indices]\n",
    "ids_val = ids[val_indices]\n",
    "obs_train = obs[train_indices]\n",
    "\n",
    "# Show the sizes of the data sets\n",
    "print(f'Size of total data set: {len(obs)}')\n",
    "print(f'Size of training set: {len(ids_train)}')\n",
    "print(f'Size of selection set: {len(ids_select)}')\n",
    "print(f'Size of validation set: {len(ids_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL5qYIvWvKK6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 3. Pre-process data\n",
    "\n",
    "This step entails transformation of the raw data to informative features that can be used by a statistical model. Standard transformations for concentration data are taking the log<sub>10</sub> and normalising by a certain variable (in this particular example: dividing all other variables by the value of Si), both of which are already applied per observation to the glass data we use.\n",
    "\n",
    "To show another example of preprocessing, in addition we here apply a z-score transformation per column to the data set: subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EfJFfyx0Zf6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The first row of observations\\n')\n",
    "print(f'   - before transformation: \\n{obs[0, :]}\\n')\n",
    "\n",
    "z_score_transformer = StandardScaler()\n",
    "z_score_transformer.fit(obs_train)\n",
    "obs_zscore = z_score_transformer.transform(obs)\n",
    "\n",
    "print(f'    - after transformation: \\n{obs_zscore[0, :]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAOrm5YJMeO-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see what this transformation does for one of the columns, that of the element K39, in the figure below. The distribution is scaled and shifted, but otherwise the same. In a plot, this manifests itself as adjusted x-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(obs[:, 0], bins=20, histtype='step', color='tab:purple', alpha=1, label='before pre-processing')\n",
    "plt.hist(obs_zscore[:, 0], bins=20, color='tab:cyan', alpha=0.5, label='after pre-processing')\n",
    "plt.xlabel('K39 value')\n",
    "plt.ylabel('count')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQjQHbFuvfCU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 4. Calculate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iiy_7IXZT59F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the previous steps, we have explored the given data, split it into subsets and performed pre-processing. We call the 10 pre-processed values we now have for each observation the features. The next step is to go from these feature vectors to a score. In order to do this we have to make pairs of observations, both *H*<sub>1</sub>-true and *H*<sub>2</sub>-true, and calculate the scores based on their feature vectors. A score is meant to quantify the degree of (dis)similarity between a pair of observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3lGrLfwRDC7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd0INeH08pYK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The function `create_pairs` below creates one *H*<sub>1</sub>-true pair per subject, and one *H*<sub>2</sub>-true pair for each pair of different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAwb3gWRcy8y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_pairs(obs, ids):\n",
    "    \"\"\"\n",
    "    Creates all possible pairs between the items represented by ids.\n",
    "    The ids refer to the total set of observations given by obs.\n",
    "    \"\"\"\n",
    "    # Create matrices with item IDs that refer to the total data set\n",
    "    H1_ids = np.transpose(np.tile(np.unique(ids), (2, 1)))\n",
    "    H2_ids = np.asarray(list(combinations(np.unique(ids), 2)))\n",
    "\n",
    "    # For H1-data: use the first repeat for each item in the first column,\n",
    "    # and the second repeat of that item in the second.\n",
    "    # We assume: that obs is sorted first by item ID, then by repeat ID;\n",
    "    # that all items have exactly 2 repeats; that there are no missing items.\n",
    "    H1_obs_rep_1 = obs[2*H1_ids[:,0] - 2]\n",
    "    H1_obs_rep_2 = obs[2*H1_ids[:,1] - 1]\n",
    "    H1_obs_pairs = np.stack((H1_obs_rep_1, H1_obs_rep_2), axis=2)\n",
    "\n",
    "    # For H2-data: use for both items their first repeats\n",
    "    H2_obs_item_1 = obs[2*H2_ids[:,0] - 2]\n",
    "    H2_obs_item_2 = obs[2*H2_ids[:,1] - 2]\n",
    "    H2_obs_pairs = np.stack((H2_obs_item_1, H2_obs_item_2), axis=2)\n",
    "\n",
    "    # Combine the H1 and H2 data, and create vector with two classes: H1 and H2\n",
    "    obs_pairs = np.concatenate((H1_obs_pairs, H2_obs_pairs))\n",
    "    hypothesis = np.concatenate((np.array(['H1']*len(H1_ids)),\n",
    "                            np.array(['H2']*len(H2_ids))))\n",
    "\n",
    "    return obs_pairs, hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWtoFN6LSKRi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We create the pairs for the subjects in the training data and the selection data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBRh4jaumhBJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "obs_pairs_train, hypothesis_train = create_pairs(obs_zscore, ids_train)\n",
    "obs_pairs_select, hypothesis_select = create_pairs(obs_zscore, ids_select)\n",
    "\n",
    "print(f'We have constructed {len(obs_pairs_train)} pairs in the training data, \\\n",
    "{int(sum(hypothesis_train=='H1'))} H1 and {int(sum(hypothesis_train=='H2'))} H2 pairs.')\n",
    "print(f'For the selection data we have {len(obs_pairs_select)} pairs, \\\n",
    "{int(sum(hypothesis_select=='H1'))} H1 and {int(sum(hypothesis_select=='H2'))} H2 pairs.\\n')\n",
    "\n",
    "print('Looking at the values for the first pair, \\n')\n",
    "print(obs_pairs_train[0,:,:])\n",
    "print('\\nwe see this is just the concatenation of the feature vectors of the \\\n",
    "two observations on the first subject:\\n')\n",
    "print(obs_zscore[:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okU1f1mLdGlU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once we have the same-source and different-source pairs we have to compute the score per pair. We show two different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reYmV7RrdIGL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option A: Compute (dis)similarity score\n",
    "We take the Manhattan distance. The Manhattan distance is the sum of the absolute differences of the features. Luckily, distances such as these are standard functions in the python package sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RupTP9OLmpb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dissimilarity_scores_train = paired_manhattan_distances(obs_pairs_train[:,:,0], obs_pairs_train[:,:,1])\n",
    "dissimilarity_scores_select = paired_manhattan_distances(obs_pairs_select[:,:,0], obs_pairs_select[:,:,1])\n",
    "\n",
    "print('For the same first pair (H1 is true) shown above, we get the following features: \\n')\n",
    "print(obs_pairs_train[0,:,:])\n",
    "\n",
    "print('\\nWe calculate the absolute differences per feature:\\n')\n",
    "print(np.absolute(np.diff(obs_pairs_train[0,:,:], axis = 1)))\n",
    "print('\\nWhen summed, we get a relatively low absolute difference of\\n')\n",
    "print(dissimilarity_scores_train[0])\n",
    "\n",
    "print('\\n\\nIf we do the same for the first H2-true pair, \\n')\n",
    "print(obs_pairs_train[hypothesis_train=='H2',:,:][0])\n",
    "print('\\nwe get a much higher distance of\\n')\n",
    "print(dissimilarity_scores_train[hypothesis_train=='H2'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_uIO4c_BWD6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More generally, we see that *H*<sub>2</sub>-true pairs get higher dissimilarity scores, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICoF_y8lcpWP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with show() as ax:\n",
    "    ax.score_distribution(scores=dissimilarity_scores_train, y=(hypothesis_train=='H1')*1,\n",
    "                          bins=np.linspace(0, 30, 60), weighted=True)\n",
    "    plt.xlabel('dissimilarity score')\n",
    "    H1_legend = mpatches.Patch(color='tab:blue', alpha=.3, label='$H_1$-true')\n",
    "    H2_legend = mpatches.Patch(color='tab:orange', alpha=.3, label='$H_2$-true')\n",
    "    ax.legend(handles=[H1_legend, H2_legend])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xhx5M4K_MN1V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option B: Compute machine learning score\n",
    "Alternatively, we can fit a statistical model to compute scores. In this case, we use a support vector machine. First, the absolute differences for each elemental value between the pairs of observations are computed. The support vector machine then assigns a score to each pair. The model aims to assign high scores to *H*<sub>1</sub>-true pairs and low scores to *H*<sub>2</sub>-true pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVYg821mJsGl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Machine learning models need a single vector as input. The AbsDiffTransformer takes two feature vectors,\n",
    "# one for each subject of a pair, and returns the elementwise absolute differences.\n",
    "# The AbsDiffTransformer and support vector machine (SVC) are combined into a single pipeline using sklearns Pipeline class.\n",
    "machine_learning_scorer = Pipeline([('abs_difference', AbsDiffTransformer()), ('classifier', SVC(probability=True))])\n",
    "\n",
    "# The model has to be fit on the data\n",
    "machine_learning_scorer.fit(obs_pairs_train, hypothesis_train=='H1')\n",
    "# Scores can be computed using the 'predict_proba' function. This is another sklearn convention,\n",
    "# which returns two columns of which we take the second using '[:,1]'\n",
    "machine_learning_scores_train = machine_learning_scorer.predict_proba(obs_pairs_train)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDPOXCifSuUS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The scores look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZbT_A_hdHE3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with show() as ax:\n",
    "    ax.score_distribution(scores=machine_learning_scores_train, y=(hypothesis_train=='H1')*1,\n",
    "                          bins=np.linspace(0, 1, 10), weighted=True)\n",
    "    plt.xlabel('machine learning score')\n",
    "    H1_legend = mpatches.Patch(color='tab:blue', alpha=.3, label='$H_1$-true')\n",
    "    H2_legend = mpatches.Patch(color='tab:orange', alpha=.3, label='$H_2$-true')\n",
    "    ax.legend(handles=[H1_legend, H2_legend])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHxmWZ_zzccC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'For example, the machine learning score for the H1-true pair from option A is \\\n",
    "{round(machine_learning_scores_train[hypothesis_train=='H1'][0],8)}, and for \\\n",
    "the first H2-true pair {round(machine_learning_scores_train[hypothesis_train=='H2'][0],8)}. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29FnyNipU05Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected, the *H*<sub>1</sub>-true pairs generally have lower dissimilarity scores than the *H*<sub>2</sub>-true pairs, while their machine learning scores are higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVUFNWS8vfVb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 5. Calculate LRs from scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W1c8Cj6dKbY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After step 4 we have a score for each pair. Two approaches exist to convert these scores to LRs. We show an example for each approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWzm5bP9gXEC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generative approach\n",
    "\n",
    "In the generative approach, we model the distribution of scores under *H*<sub>1</sub> and under *H*<sub>2</sub>. Here, we use kernel density estimation (KDE) to do this. KDE is a way to estimate an unknown probability density by 'smoothing' the observed distribution. LiR defines classes called 'Calibrators' to perform the score to LR mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KbSziDavfcq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kde_calibrator = KDECalibrator(bandwidth='silverman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "femRMlnRoPQS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To illustrate how this works, we plot the histograms for the scores under *H*<sub>1</sub> and *H*<sub>2</sub>-true, together with the KDE fits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0Vtymxk5M-m",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kde_calibrator.fit(dissimilarity_scores_train, hypothesis_train=='H1')\n",
    "with show() as ax:\n",
    "    ax.calibrator_fit(kde_calibrator, score_range=[0,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1EH61o0oNba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kde_calibrator.fit(dissimilarity_scores_train, hypothesis_train=='H1')\n",
    "with show() as ax:\n",
    "    ax.calibrator_fit(kde_calibrator, score_range=[0, 30])\n",
    "    ax.score_distribution(scores=dissimilarity_scores_train, y=(hypothesis_train=='H1')*1,\n",
    "                          bins=np.linspace(0, 30, 60), weighted=True)\n",
    "    ax.xlabel('dissimilarity score')\n",
    "    H1_legend = mpatches.Patch(color='tab:blue', alpha=.3, label='$H_1$-true')\n",
    "    H2_legend = mpatches.Patch(color='tab:orange', alpha=.3, label='$H_2$-true')\n",
    "    ax.legend(handles=[H1_legend, H2_legend])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVfefhuWA4Nn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Discriminative approach\n",
    "Alternatively, we can use other discriminative methods. Here we use logistic regression to map the score to posterior odds. This can be corrected for the amount of *H*<sub>1</sub> and *H*<sub>2</sub> data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nii8tmG8g_T7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logreg_calibrator = LogitCalibrator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AJrm1R-YZN-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bounding the LRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtD_6oAQpKyY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The LR system may output values that seem too large or too small given the size of our data set. Several methods have been proposed mitigate this problem, by 'shrinking' the LRs towards 1. Here we show how to use empirical lower and upper bounds (ELUB) to limit the minimum and maximum values the LR system can output. Using LiR, we can stack this 'ELUBbounder' on any calibrator we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2Qjs4lNh1Oq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bounded_kde_calibrator = ELUBBounder(kde_calibrator)\n",
    "bounded_logreg_calibrator = ELUBBounder(logreg_calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y0ANL6KohdA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Illustration of scores to LRs\n",
    "To illustrate the score to LR mapping, we plot how the dissimilarity scores are mapped to LRs (using a log<sub>10</sub> scale). We print the values of the ELUB bounds - these are visible in the plot as the minimum and maximum values the LRs obtain (horizontal lines).\n",
    "\n",
    "One thing to note is that the purple (logistic regression) never goes up. This is a desirable property, it makes sense that more extreme scores correspond to more extreme LRs. In contrast, the cyan line (kernel density estimation (KDE)) shows several humps, e.g. at score=0 and score=2. This means that, counterintuitively, sometimes a higher dissimilarity leads to a lower LR. Here we don't 'fix' this (e.g. by adjusting the bandwidth of the KDE), but be aware that some methods for transforming scores to LRs can have this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcNgO354aNRP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lrs_train_logreg = bounded_logreg_calibrator.fit_transform(dissimilarity_scores_train, hypothesis_train=='H1')\n",
    "lrs_train_kde = bounded_kde_calibrator.fit_transform(dissimilarity_scores_train, hypothesis_train=='H1')\n",
    "\n",
    "plt.scatter(dissimilarity_scores_train, lrs_train_logreg, color='tab:purple', marker='.', label='logistic regression')\n",
    "plt.scatter(dissimilarity_scores_train, lrs_train_kde, color='tab:cyan', label = 'KDE')\n",
    "plt.xlim([0, 5])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('dissimilarity score')\n",
    "plt.ylabel('log$_{10}$(LR)')\n",
    "\n",
    "print(f'ELUB log(LR) bounds for logistic reggression are {(bounded_logreg_calibrator.lower_llr_bound):.2f} and \\\n",
    "{(bounded_logreg_calibrator.upper_llr_bound):.2f}')\n",
    "print(f'ELUB log(LR) bounds for KDE are {(bounded_kde_calibrator.lower_llr_bound):.2f} and \\\n",
    "{(bounded_kde_calibrator.upper_llr_bound):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL_ZPG0xZ4Yk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that for the generative approach, the score to LR function is not monotonous. This is something we do not expect from a score to LR function, and possibly could be solved by using a kernel density estimator with a greater bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm940saTvtoH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 6. Select best LR system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnvy3CU1yhF2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this step, we compute the performance of each LR system defined on the selection data. For this, we use the CalibratedScorer class from LiR, which allows you to combine computing scores and transforming to LRs in one step. A CalibratedScorer needs two parts: a function or machine learning model to compute scores, and a Calibrator to transform the scores to LRs.\n",
    "\n",
    "We define a function to give us the histograms of the LRs, the PAV plot, the ELUB bounds and the *C*<sub>llr</sub>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozHVgS_YUXWG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_performance(lrs, hypothesis, calibrator):\n",
    "    hypothesis=(hypothesis=='H1')*1 # Convert to 0 or 1 for technical reasons\n",
    "\n",
    "    # Show the distribution of LRs together with the ELUB values\n",
    "    print('Histogram of the LRs:\\n')\n",
    "\n",
    "    with show() as ax:\n",
    "        ax.lr_histogram(lrs, hypothesis)\n",
    "        H1_legend = mpatches.Patch(color='tab:blue', alpha=.25, label='$H_1$-true')\n",
    "        H2_legend = mpatches.Patch(color='tab:orange', alpha=.25, label='$H_2$-true')\n",
    "        ax.legend(handles=[H1_legend, H2_legend])\n",
    "\n",
    "    print(f'\\nELUB log(LR) bounds are {np.log10(calibrator._lower_lr_bound):.2f} and\\\n",
    "    {np.log10(calibrator._upper_lr_bound):.2f} \\n')\n",
    "\n",
    "    print('PAV plot (closer to the line y=x is better):\\n')\n",
    "    # Show the PAV plot (closer to the line y=x is better)\n",
    "    with show() as ax:\n",
    "        ax.pav(lrs, hypothesis)\n",
    "\n",
    "    # Print the quality of the system as log likelihood ratio cost (lower is better)\n",
    "    print(f'\\n The log likelihood ratio cost is {lir.metrics.cllr(lrs, hypothesis):.3f} (lower is better)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIbmOuuXjBwD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option 1: Manhattan distance + KDE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raEanCwsvfto",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We define the CalibratedScorer. We use the standard sklearn function\n",
    "# paired_manhattan_distance to compute the scores, and an ELUB bounded\n",
    "# KDE calibrator to transform to LRs.\n",
    "lr_system = lir.CalibratedScorer(paired_manhattan_distances, bounded_kde_calibrator)\n",
    "# We fit the entire LR system\n",
    "lr_system.fit(obs_pairs_train, hypothesis_train=='H1')\n",
    "\n",
    "# We compute the LRs on the selection data\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "# This is how well the system performs\n",
    "show_performance(lrs_select, hypothesis_select, lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KOiJLt2qdDx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option 2: Manhattan distance + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjj5RdAHqWv5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We define the CalibratedScorer. We use the standard sklearn function paired_manhattan_distance to compute the scores,\n",
    "# and an ELUB bounded logistic regression calibrator to transform to LRs.\n",
    "lr_system = lir.CalibratedScorer(paired_manhattan_distances, bounded_logreg_calibrator)\n",
    "# We fit the entire LR system\n",
    "lr_system.fit(obs_pairs_train, hypothesis_train=='H1')\n",
    "\n",
    "# We compute the LRs on the selection data\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "# This is how well the system performs\n",
    "show_performance(lrs_select, hypothesis_select, lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eUrLzX2rqfZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option 3: support vector machine + KDE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JePlY8kztDjv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The whole LR system consists of computing the score and then mapping the scores to LRs.\n",
    "# We use the machine learning scorer introduced in step 4\n",
    "scorer = Pipeline([('abs_difference', AbsDiffTransformer()), ('classifier', SVC(probability=True))])\n",
    "\n",
    "lr_system = lir.CalibratedScorer(scorer, bounded_kde_calibrator)\n",
    "\n",
    "# We fit the whole system. When fitting (=training) a CalibratedScorer, both the\n",
    "# machine learning model and the transformation to LRs are trained on the supplied data\n",
    "lr_system.fit(obs_pairs_train, hypothesis_train=='H1')\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "show_performance(lrs_select, hypothesis_select, lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu4Bcbsss_fI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Option 4: support vector machine + logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xs47hMySrmqG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The whole LR system consists of computing the score and then mapping the scores to LRs\n",
    "scorer = Pipeline([('abs_difference', AbsDiffTransformer()), ('classifier', SVC(probability=True))])\n",
    "\n",
    "lr_system = lir.CalibratedScorer(scorer, bounded_logreg_calibrator)\n",
    "# We fit the whole system\n",
    "lr_system.fit(obs_pairs_train, hypothesis_train=='H1')\n",
    "lrs_select = lr_system.predict_lr(obs_pairs_select)\n",
    "\n",
    "show_performance(lrs_select, hypothesis_select, lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YljF4uSbuHMI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Final selection\n",
    "All four options show reasonable LR distributions. Because the glass data contain much information, the *H*<sub>1</sub> and *H*<sub>2</sub> are almost perfectly separated. Most of the *H*<sub>2</sub> pairs, and many of the *H*<sub>1</sub> pairs fall on the ELUB bounds. We see similar performance across the four systems. The LR distributions look reasonable for all systems, the PAV plots look good for options 2 and 3 (with the points following the line y=x) and show slightly conservative LRs for options 1 and 4 (points lying above the line y=x).\n",
    "\n",
    "We have a slight preference for option 3, which has a good PAV plot, the best (=lowest) *C*<sub>llr</sub> and best (=widest) ELUB bounds. Option 2 would also have been a reasonable choice, particularly as the Manhattan distance is much more explainable than the support vector machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyaszrzmvf1r",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 7. Validate selected LR system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wNu8nK75EcP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After selecting the (best) model, you should assess the performance of the system on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUmGVlDO_GbZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Construct the selected system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu4twJ90_N54",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we want to build the selected model based on as much data as possible, we add the selection set to the training set and use the combination to train the LR system used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TU1Ee4PxSOBa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the combined data set\n",
    "obs_train_select = obs[train_select_indices]\n",
    "ids_train_select = ids[train_select_indices]\n",
    "\n",
    "# Step 3 pre-processing: normalise\n",
    "z_score_transformer.fit(obs_train_select)\n",
    "obs_zscore = z_score_transformer.transform(obs)\n",
    "\n",
    "# Step 4: combine the pairs into one feature vector by taking the absolute difference\n",
    "obs_pairs_train_select, hypothesis_train_select = create_pairs(obs_zscore, ids_train_select)\n",
    "obs_pairs_val, hypothesis_val = create_pairs(obs_zscore, ids_val)\n",
    "\n",
    "selected_lr_system = lir.CalibratedScorer(machine_learning_scorer,\n",
    "                                          lir.ELUBbounder(lir.KDECalibrator(bandwidth='silverman')))\n",
    "# Step 4+5 combined: we fit the whole system\n",
    "selected_lr_system.fit(obs_pairs_train_select, hypothesis_train_select=='H1')\n",
    "\n",
    "# Compute the LRs on the validation data\n",
    "lrs_val = selected_lr_system.predict_lr(obs_pairs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4z8tJgF_P54",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate system performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5BmurxZvf-8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We always inspect the characteristics we also look at in selection\n",
    "show_performance(lrs_val, hypothesis_val, selected_lr_system.calibrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgGqN6o46BqD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# There are many other characteristics that we may want to inspect,\n",
    "# such as the empirical cross entropy (ECE) plot\n",
    "with show() as ax:\n",
    "      ax.ece(lrs_val, hypothesis_val=='H1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUWNjDBL_Cez",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the validation, we hope to see performance comparable to that on the selection data. This is the case here, with *C*<sub>llr</sub>, ELUB bounds and LR distributions looking very similar to those we saw in step 6. The PAV plot looks reasonable, with the data points mostly following the line y=x. There are a few datapoints that lie under this line, indicating a slight overconfidence of the system in this LR range. The ECE plot in Figure shows that the LRs are, for any prior odds, performing substantially better than a system where LR=1 (the reference system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foIk-OxuOg1A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 8. Construct casework LR system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er-ykWWV_fEI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To obtain our casework LR system, we fit on all available data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AditaVUo-1DU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create new pairs for the combined data\n",
    "obs_zscore = z_score_transformer.fit_transform(obs)\n",
    "obs_pairs, hypothesis = create_pairs(obs_zscore, ids)\n",
    "\n",
    "# Fit the same system on all the data\n",
    "selected_lr_system.fit(obs_pairs, hypothesis=='H1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z37mueAI_9pw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that we cannot empirically test this system. In casework, if you have decided to use the system, you would input your pair of observation and get back an LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb40EneuALen",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "observation_1 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "observation_2 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "manually_entered_data = np.moveaxis(np.array([z_score_transformer.transform([observation_1,observation_2])]),1,-1)\n",
    "\n",
    "print(f'The LR obtained is {selected_lr_system.predict_lr(manually_entered_data)[0]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
