# score-based-common-source.yaml - a template yaml for setting up an LR system and a validation experiment.
#
# Below is a complete working example. If you want to adapt the setup to your own needs, you may want to specify:
#
# 1. Output folder
# 2. Path to data & how to parse it
# 3. How to assign training and testing roles to your data
# 4. The design of the lr system itself
# 5. The validation experiment you run for your lr system
#
# When you are done, you can run it all using:
# pdm run lir --setup examples/score-based-common-source.yaml
#

# 1. OUTPUT FOLDER
# You will create an LR system and design a validation experiment. The output of the experiment will be written to the folder specified below.
# The output will be intermediate results, metrics and graphical representations. 
# There will be a sub folder for each experiment. The sub folders each contain a file named `metrics.csv` which contains the metrics for each
# LR system that is tested, and for each LR system there is a folder that may contain visualizations and other by products.
# It is great practice to use a timestamp in the folder name so it is easier to keep track of experiment results (and to avoid overwriting earlier results).

output_path: output/${timestamp}_score_based_common_source  # <-- adapt this value to your preference 

# 2. PATH TO DATA AND HOW TO PARSE IT
# An example dataset of LA-ICP-MS measurements of elemental concentration from floatglass is provided as a default dataset. For more
# information on the dataset, see: https://github.com/NetherlandsForensicInstitute/elemental_composition_glass
#
# This example is only provided to give working default values for the information you need to enter for your own data. 
# You should give a path to your data. If your data is in the default format, the default parser should work.
# If your data is not in the default format you can either write a new parser (the hard option) or, if possible, reformat your data (the easy option).
# If you want to know how to use the default csv parser, go here. <TODO>

data:
  provider: feature_data_csv_http
  url: https://raw.githubusercontent.com/NetherlandsForensicInstitute/elemental_composition_glass/main/training.csv
  instance_id_column: id
  source_id_column: Item
  ignore_columns:
    - Piece

#3. ASSIGNING TEST AND TRAIN DATA ROLES
# This will describe which data is used to train the system and which to test (validate) the system. 

# You may choose a cross-design, in which the data is divided into folds. Each fold is used as validation data sequentially,
# and for each of the folds an LR system is trained with all the other folds. The outcomes for all folds are then pooled into one set.

data_roles:                                # <-- adapt this name to your own preference, you will need it later when setting up the validation experiment
  strategy: multiclass_cross_validation
  folds: 5

  # Alternatively, you may choose a simple split, where some data is assigned the role of training data and some is assigned the role of validation data.
  # If you want that, you use 'multiclass_train_test_split' for 'strategy' and you specify what the size of the test data with 'test_size'.
  # All the rest will be used as training data. The test size can be a float between 0 and 1, which will be used as a percentage.
  # It can also be an integer, that will be used as an absolute number of sources. 
  # You also specify a seed for the random assignment of sources to the train or test category.
  # If you want this, uncomment the next 3 lines and comment out the two lines above. 

  #strategy: multiclass_train_test_split
  #seed: 42
  #test_size: 0.3


# 4. LR SYSTEM DESIGN
# Next, the score-based common source LR system will be defined.

score_based_lr_system:        # <-- this is a custom name of your LR system that you can adapt. It will be referred to later.
  intermediate_output: True   # <-- if you want csv output for every step in your LR system, use 'True'. Otherwise, use 'False'
  architecture: score_based   # Since this is the yaml for a score based system, we specify the architecture to be 'score_based'.
  
  # This 'score-based' architecture takes three arguments:
  # - `preprocessing`: a series of steps that take your features as input and do some preprocessing, before pairs are formed;
  # - `pairing`: a pairing method that takes instances as input and produces same-source and different-source pairs;
  # - `comparing`: a series of steps that takes the pairs as input and produces LLRs.
  #
  # A step may be a scikit-learn-style transformer, an estimator or a function.
  # A step has a name that you give it and specifies a `method` argument which defines the module class,
  # which may be a scikit-learn-style transformer, estimator or a plain function. 
  # All other arguments are passed to the module class on instantiation, like this:
  #
  # scaler:                        <-- a, custom, descriptive name of your step, used to refer to later on
  #   method: standard_scaler      <-- the name of the transformer / estimator / function
  #   with_mean: False             <-- any arguments your specific function needs
  #
  # Alternatively, if the module needs no arguments, the module class can be entered directly. The modules `scaler1` and
  # `scaler2` in the following snippet are equivalent.
  #
  # scaler1:                      <-- the name of your step
  #   method: standard_scaler     <-- the pointer to the right function
  #
  # scaler2: standard_scaler      <-- the notation shortcut that is possible because no arguments are needed
  #
  
  preprocessing:             
  # LOGGING NU NAAR BOVEN GEMIGREERD, CHECKEN OF DAT NU WEL KON <TODO>
  # define your preprocessing pipeline here by adding steps.
  # This is before pairing, so the steps should be applicable per instance.
  
    steps:
      scaler: standard_scaler  # apply the scikit-learn `StandardScaler` with default arguments

  pairing:
  # define the way you want to pair your data. UITLEG MOGELIJKE PAIRING METHODES <TODO>
    method: instance_pairs
    ratio_limit: 2  # limit the number of different-source pairs to the number of same-source pairs

  comparing:
  # define the way you want to make scores from your features and to make LLRs from those scores.
  # This is after pairing, so the steps should be applicable per pair.
    steps:
      scoring: abs_diff                         # calculate the absolute difference between the two instances for each feature
      clf: logistic_regression                  # use scikit-learn `LogisticRegression` to estimate probabilities
      to_llr: probabilities_to_logodds          # convert probabilities to their log-odds values
      calibration:                              
        method: kde
        bandwidth: silverman
                                                # The three lines above calibrate the log-odds to LLRs using KDE.
                                                # Another sensible choice is 'logistic_regression', there you can suffice with 'calibration: logistic_regression'
      bounding: elub_bounder                    # prevent excessive extrapolation by applying an LLR bounder

# You have defined your LR system. Below we define how the LR system should be evaluated.
# It is possible to set up experiments where multiple design choices are involved, to enable parameter and model selection.
# To find out how more complex designs should be set up, see here LINK <TODO>
# For now, we define a single simple validation experiment:

# 5. THE VALIDATION EXPERIMENT

experiments:                             
  - name: my_validation_run              # This is the name of the experiment, use your own preferred value.
    strategy: single_run                 # For a single validation experiment, use 'single_run'.
    lr_system: ${score_based_lr_system}  # This refers to the lr system defined above, make sure to use the name you used above.
    data: ${data}                        # This refers to the data defined above, make sure to use the name you used above.
    data_roles: ${data_roles}            # This refers to the data roles defined above, make sure to use the name you used above.
    output:                              # Here you define what output you want. The output will at least include a copy of this yaml and statistics like number of sources and instances.
      - metrics                          # If you want to know more about the types of output that are possible, go HERE <TODO>
      - pav
      - ece
      - save_model
      - lr_histogram
      
  
